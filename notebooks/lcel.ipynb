{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Expression Language (LCEL)\n",
    "\n",
    "In this notebook, we will give a high level introduction to what **LangChain Expression Language (LCEL)** is and show we can use it with an MLX locally-deployed model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "Throughout this notebook, we will largely be making use of LangChain alongside MLX. In order to do a direct comparison with how MLX works within LangChain, we will also provide an example using the standard OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary Python libraries\n",
    "import yaml\n",
    "from mlx_lm import load, generate\n",
    "from typing import Any, List, Mapping, Optional\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Importing legacy LangChain things for demonstration purposes\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting constant values to represent model name and directory\n",
    "MODEL_NAME = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "BASE_DIRECTORY = '../models'\n",
    "MLX_DIRECTORY = f'{BASE_DIRECTORY}/mlx'\n",
    "mlx_model_directory = f'{MLX_DIRECTORY}/{MODEL_NAME}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading my personal OpenAI API key (NOT pushed up to GitHub)\n",
    "with open('../sensitive/api-keys.yaml') as f:\n",
    "    API_KEYS = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining the Old Way\n",
    "In the cell below, we will demonstrate the former way of chaining a prompt template together with an LLM. LangChain has offered multiple ways to chain prompts to LLMs, but perhaps one of the most popular ways was using the `ConversationChain`. Let's go ahead and set up a simple prompt template and chain that to the OpenAI API to demonstrate how this legacy option worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the OpenAI LLM\n",
    "llm = ChatOpenAI(openai_api_key = API_KEYS['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the Chat prompt template\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template = 'You are a helpful assistant.')\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(template = \"History: {history}\\nHuman: {input}\")\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the legacy conversation chain\n",
    "legacy_conversation_chain = ConversationChain(llm = llm, prompt = chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blossoms in the sun,\n",
      "Colors paint the world with joy,\n",
      "Nature's gift of love.\n"
     ]
    }
   ],
   "source": [
    "# Making a call to the OpenAI API\n",
    "response = legacy_conversation_chain.predict(input = 'Write me a haiku about flowers.')\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why LCEL?\n",
    "\n",
    "As you can see, the legacy syntax has historically not been the easiest to learn. In addition to the `ConversationChain` object, LangChain has provided many other objects that perform more or less the same thing, introducing confusion on when to use what.\n",
    "\n",
    "Additionally, what is not particularly clear by using the `ConversationChain` object, but there is a simple memory that is being kept each time that object is called. This may or may not be preferable given your use case, but in any regard, it is not ideal that this is abstracted away.\n",
    "\n",
    "LCEL attempts to simplify the process by allowing users to chain LangChain objects together directly using the pipe (`|`) delimiter. There are many benefits to using LCEL, and you can read more about them [at this page](https://python.langchain.com/docs/expression_language/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the OpenAI LLM\n",
    "llm = ChatOpenAI(openai_api_key = API_KEYS['OPENAI_API_KEY'])\n",
    "\n",
    "# Setting up the Chat prompt template\n",
    "chat_prompt = ChatPromptTemplate.from_template('{input}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the new chain with the LCEL syntax\n",
    "new_conversation_chain = chat_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Petals soft and bright\n",
      "Dancing in the gentle breeze\n",
      "Nature's beauty glows\n"
     ]
    }
   ],
   "source": [
    "# Invoking the model appropriately\n",
    "response = new_conversation_chain.invoke({'input': 'Write me a haiku about flowers.'})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using MLX with LCEL\n",
    "By default, there is currently no mechanism within the LangChain libraries that supports MLX; however, we still can make use of MLX in a custom capacity. In the next few cells, we will demonstrate how we can do this appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the quantized model and tokenizer with MLX\n",
    "model, tokenizer = load(mlx_model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a class to represent our custom MLX LLM\n",
    "class MLX_LLM(LLM):\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return 'custom'\n",
    "    \n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None,):\n",
    "        response = generate(\n",
    "            model = model,\n",
    "            tokenizer = tokenizer,\n",
    "            prompt = prompt,\n",
    "            max_tokens = 1000\n",
    "        )\n",
    "        \n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = MLX_LLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_chain = chat_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'content'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Invoking the model appropriately\u001b[39;00m\n\u001b[1;32m      2\u001b[0m response \u001b[38;5;241m=\u001b[39m new_chain\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWrite me a haiku about flowers.\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'content'"
     ]
    }
   ],
   "source": [
    "# Invoking the model appropriately\n",
    "response = new_chain.invoke({'input': 'Write me a haiku about flowers.'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nSure, here's a simple haiku about flowers:\\n\\nPetals bloom, soft breeze,\\nWhispers life in gentle hue,\\nSpring's sweet promise sings.\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
