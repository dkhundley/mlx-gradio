{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Expression Language (LCEL)\n",
    "\n",
    "In this notebook, we will give a high level introduction to what **LangChain Expression Language (LCEL)** is and show we can use it with an MLX locally-deployed model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "Throughout this notebook, we will largely be making use of LangChain alongside MLX. In order to do a direct comparison with how MLX works within LangChain, we will also provide an example using the standard OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing the necessary Python libraries\n",
    "import yaml\n",
    "from typing import Any, Dict, List, Optional\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.pydantic_v1 import root_validator, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import BaseMessage, AIMessage\n",
    "from langchain_core.outputs import ChatGeneration, ChatResult\n",
    "from mlx_lm import load as mlx_load\n",
    "from mlx_lm import generate as mlx_generate\n",
    "\n",
    "# Importing legacy LangChain things for demonstration purposes\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting constant values to represent model name and directory\n",
    "MODEL_NAME = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "BASE_DIRECTORY = '../models'\n",
    "MLX_DIRECTORY = f'{BASE_DIRECTORY}/mlx'\n",
    "mlx_model_directory = f'{MLX_DIRECTORY}/{MODEL_NAME}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading my personal OpenAI API key (NOT pushed up to GitHub)\n",
    "with open('../sensitive/api-keys.yaml') as f:\n",
    "    API_KEYS = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining the Old Way\n",
    "In the cell below, we will demonstrate the former way of chaining a prompt template together with an LLM. LangChain has offered multiple ways to chain prompts to LLMs, but perhaps one of the most popular ways was using the `ConversationChain`. Let's go ahead and set up a simple prompt template and chain that to the OpenAI API to demonstrate how this legacy option worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the OpenAI LLM\n",
    "llm = ChatOpenAI(openai_api_key = API_KEYS['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the Chat prompt template\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template = 'You are a helpful assistant.')\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(template = \"History: {history}\\nHuman: {input}\")\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the legacy conversation chain\n",
    "legacy_conversation_chain = ConversationChain(llm = llm, prompt = chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Petals soft and bright,\n",
      "Dancing in the gentle breeze,\n",
      "Nature's sweet delight.\n"
     ]
    }
   ],
   "source": [
    "# Making a call to the OpenAI API\n",
    "response = legacy_conversation_chain.predict(input = 'Write me a haiku about flowers.')\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why LCEL?\n",
    "\n",
    "As you can see, the legacy syntax has historically not been the easiest to learn. In addition to the `ConversationChain` object, LangChain has provided many other objects that perform more or less the same thing, introducing confusion on when to use what.\n",
    "\n",
    "Additionally, what is not particularly clear by using the `ConversationChain` object, but there is a simple memory that is being kept each time that object is called. This may or may not be preferable given your use case, but in any regard, it is not ideal that this is abstracted away.\n",
    "\n",
    "LCEL attempts to simplify the process by allowing users to chain LangChain objects together directly using the pipe (`|`) delimiter. There are many benefits to using LCEL, and you can read more about them [at this page](https://python.langchain.com/docs/expression_language/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the OpenAI LLM\n",
    "llm = ChatOpenAI(openai_api_key = API_KEYS['OPENAI_API_KEY'])\n",
    "\n",
    "# Setting up the Chat prompt template\n",
    "chat_prompt = ChatPromptTemplate.from_template('{input}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the new chain with the LCEL syntax\n",
    "new_conversation_chain = chat_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clumsy Gungan friend\n",
      "Jar Jar brings laughter and joy\n",
      "In a galaxy\n"
     ]
    }
   ],
   "source": [
    "# Invoking the model appropriately\n",
    "response = new_conversation_chain.invoke({'input': 'Write me a haiku about Jar Jar Binks.'})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using MLX with LCEL\n",
    "By default, there is currently no mechanism within the LangChain libraries that supports MLX; however, we still can make use of MLX in a custom capacity. In the next few cells, we will demonstrate how we can do this appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the class representing the MLX Chat Model, inheriting from LangChain's BaseChatModel\n",
    "class MLXChatModel(BaseChatModel):\n",
    "    mlx_path: str\n",
    "    mlx_model: Any = Field(default = None, exclude = True)\n",
    "    mlx_tokenizer: Any = Field(default = None, exclude = True)\n",
    "    max_tokens: int = Field(default = 1000)\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return 'MLXChatModel'\n",
    "    \n",
    "\n",
    "\n",
    "    @root_validator()\n",
    "    def load_model(cls, values: Dict) -> Dict:\n",
    "\n",
    "        # Loading the model and tokenizer with the input string\n",
    "        model, tokenizer = mlx_load(path_or_hf_repo = values['mlx_path'])\n",
    "        \n",
    "        # Saving the variables back appropriately\n",
    "        values['mlx_model'] = model\n",
    "        values['mlx_tokenizer'] = tokenizer\n",
    "        return values\n",
    "    \n",
    "\n",
    "\n",
    "    def _generate(self, messages: List[BaseMessage], stop: Optional[List[str]]) -> ChatResult:\n",
    "\n",
    "        # Instantiating an empty string to represent the prompt we will be generating in the end\n",
    "        prompt = ''\n",
    "\n",
    "        # Extracting the raw text from each of the LangChain message types\n",
    "        for message in messages:\n",
    "            prompt += f'\\n\\n{message.content}'\n",
    "\n",
    "        # Generating the LLM response using MLX\n",
    "        mlx_response = mlx_generate(\n",
    "            model = self.mlx_model,\n",
    "            tokenizer = self.mlx_tokenizer,\n",
    "            max_tokens = self.max_tokens,\n",
    "            prompt = prompt\n",
    "        )\n",
    "\n",
    "        # Returning the MLX response as a proper LangChain ChatResult object\n",
    "        return ChatResult(generations = [ChatGeneration(message = AIMessage(content = mlx_response))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the MLX model using our quantized Mistral 7B\n",
    "mlx_model = MLXChatModel(mlx_path = mlx_model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the Chat prompt template\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template = 'You are a helpful assistant.')\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(template = \"{input}\")\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the MLX chain with the LCEL syntax\n",
    "mlx_chain = chat_prompt | mlx_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Giddy, clumsy Gungan,\n",
      "Bouncing through Star Wars' world,\n",
      "Laughter, then disdain.\n"
     ]
    }
   ],
   "source": [
    "# Generating the response with MLX + LCEL\n",
    "response = mlx_chain.invoke(input = {'input': 'Write a haiku about Jar Jar Binks.'})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together\n",
    "Now that we've demonstrated how we can use LangChain to very rapidly swap out various models so that we can do quick testing. For our test below, I'm going to use 3 models: ChatGPT as provided by OpenAI, Mixtral as provided by Perplexity, and my local Mistral provided by MLX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the Chat prompt template\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template = 'You are a helpful assistant.')\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(template = \"{input}\")\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# Setting an input prompt\n",
    "input_prompt = {'input': 'Write a haiku about Jar Jar Binks.'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently testing the following model: OpenAI\n",
      "Mesa clumsy friend,\n",
      "Jar Jar dances through the swamp,\n",
      "Gungans sing his name.\n",
      "\n",
      "Currently testing the following model: Perplexity (Mixtral)\n",
      "Gungan in a bind,\n",
      "Clumsy words, but heart kind,\n",
      "Jar Jar, hard to hate.\n",
      "\n",
      "Currently testing the following model: MLX (Mistral)\n",
      "\n",
      "\n",
      "Giddy, clumsy Gungan,\n",
      "Bouncing through Star Wars' world,\n",
      "Laughter, then disdain.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterating over each of our 3 models\n",
    "for model_type in ['OpenAI', 'Perplexity (Mixtral)', 'MLX (Mistral)']:\n",
    "    \n",
    "    print(f'Currently testing the following model: {model_type}')\n",
    "\n",
    "    # Loading the respective model\n",
    "    if model_type == 'OpenAI':\n",
    "        model = ChatOpenAI(openai_api_key = API_KEYS['OPENAI_API_KEY'])\n",
    "    elif model_type == 'Perplexity (Mixtral)':\n",
    "        model = ChatOpenAI(api_key = API_KEYS['PERPLEXITY_API_KEY'],\n",
    "                           base_url = 'https://api.perplexity.ai',\n",
    "                           model = 'mixtral-8x7b-instruct')\n",
    "    elif model_type == 'MLX (Mistral)':\n",
    "        model = MLXChatModel(mlx_path = mlx_model_directory)\n",
    "\n",
    "    # Instantiating the chain with the LCEL syntax\n",
    "    llm_chain = chat_prompt | model\n",
    "\n",
    "    # Invoking the chain\n",
    "    response = llm_chain.invoke(input = input_prompt)\n",
    "    print(response.content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class MLXModelParameters():\n",
    "\n",
    "    def __init__(self, temp = 0.7, max_tokens = 1000):\n",
    "        self.temp = temp\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'Temperature: {self.temp}\\nMax Tokens: {self.max_tokens}'\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'Temperature: {self.temp}\\nMax Tokens: {self.max_tokens}'\n",
    "\n",
    "    def update_temp(self, new_temp):\n",
    "        self.temp = new_temp\n",
    "\n",
    "    def update_max_tokens(self, new_max_tokens):\n",
    "        self.max_tokens = new_max_tokens\n",
    "\n",
    "    def to_json(self):\n",
    "        return { 'temp': self.temp, 'max_tokens': self.max_tokens }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = MLXModelParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params.update_temp(new_temp = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 1\n",
      "Max Tokens: 1000\n"
     ]
    }
   ],
   "source": [
    "print(model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'temp': 1, 'max_tokens': 1000}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_params.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(file = '../data/schema.json') as f:\n",
    "    test_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a helpful assistant.'},\n",
       " {'role': 'user', 'content': 'What is the weather like today?'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'It will be a nice day today.',\n",
       "  'metadata': {'model_name': 'mistral_7b',\n",
       "   'meta_prompt': 'You are a helpful assistant.',\n",
       "   'temperature': 0.7,\n",
       "   'max_tokens': 1000,\n",
       "   'like_data': None}}]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_json['chat_history'][0]['chat_interaction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
