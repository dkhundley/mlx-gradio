{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Quantization with MLX\n",
    "In this notebook, we will learn the basics of **quantization** and why it can be important in the context of large language mdoels (LLMs). We'll start more generically by demonstrating how to perform quantization in general, and then we'll move into quantization using Apple's new Python framework, MLX.\n",
    "\n",
    "## Why Quantization?\n",
    "When an LLM is trained, the weights of the model are generally stored in a float datatype, particularly `float32`. Float values are very precise, but they can be computationally inefficient to calculate. This is because `float32` can span a very wide range of values. This precision can be important to get the most accuracy out of an LLM, but in addition to computational inefficiency, this also means that storing the weights of a model can make the file sizes very large.\n",
    "\n",
    "Let's now compare this with a smaller datatype, like `int8`. `int8` has a much smaller span of values, ranging from -127 to 127. What people have discovered is that we can rather effectively map something like the wide range of `float32` values to this much narrower set of `int8` values without degrading the model performance too much. Additionally, it shrinks the storage size of the model considerably to something that can run on smaller hardware.\n",
    "\n",
    "For example, let's say we had the following four `float32` values.\n",
    "\n",
    "- 0.1357\n",
    "- -0.9875\n",
    "- 0.5432\n",
    "- -0.3214\n",
    "\n",
    "If we were to map these values to the `int8` datatype, they would transform the following way:\n",
    "\n",
    "- 13\n",
    "- -98\n",
    "- 54\n",
    "- -32\n",
    "\n",
    "Again, we are indeed effectively losing information here, but as you'll discover, this trade off may be acceptable for our purposes. Namely, running an LLM on a cloud GPU can be very costly, so using your own personal hardware (aka a MacBook) to run a quantized LLM can save you quite the cost!\n",
    "\n",
    "Throughout this notebook, we will be making use of the open weight model [**Mistral 7B**](https://mistral.ai/news/announcing-mistral-7b/). We will specifically be making use of it from the Hugging Face Hub, downloading the model in its \"raw\" state and then later quantizing that for usage on a MacBook for inference purposes.\n",
    "\n",
    "\n",
    "\n",
    "## YMMV (Your Mileage May Vary)\n",
    "In case you are curious, I personally am working on a MacBook Pro with an M1 Pro chip with 16gb of RAM (memory). This is the \"baseline\" 16 inch MacBook Pro model that was for sale a few years ago, and Apple has since developed newer models with even more powerful M2 and M3 chips. (Note: MLX is specifically designed for these newer Apple Silicon Macs and will NOT work with older Intel-based Macs.)\n",
    "\n",
    "The reason I share this is because MLX will work on any Apple Silicon Mac, but the hardware restrictions limit what you can effectively do with MLX. For example, I attempted to download Mistral's larger [Mixtral](https://mistral.ai/news/mixtral-of-experts/) model. Mixtral is significantly larger than Mistral. Whereas Mistral 7B is roughly a 28GB download, Mixtral is a whopping 100GB. When I attempted to quantize Mixtral on my own hardware, my hardware kept crapping out. ðŸ˜… This is why we'll be working with Mistral 7B, since it is smaller and can run more effectively on hardware such as mine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "Let's begin by talking about the Python libraries that we'll be installing / using throughout this notebook.\n",
    "\n",
    "- `transformers`: This is the primary Python library supported by Hugging Face. While `transformers` is great for many purposes, the sole reason we'll be using it in this notebook is to simply download the tokenizer and model for Mistral 7B.\n",
    "- `mlx`: This is the base ML framework created by Apple to run advanced predictive models and more directly on Apple hardware. While we will be using a MacBook in our case, MLX also has a flavoring in Swift, meaning that you can effectively use MLX to run AI models on iPhones and iPads! While I have not created an app like this myself, there are actually apps that you can purchase on the App Store right now that will run a quantized version of Mistral 7B. These apps seem more sandboxy in nature, so I won't give any particular recommendations. But it's nice to know this is possible!\n",
    "- `mlx-lm`: Knowing that people are very interested in running LLMs on Mac hardware, Apple created a subsidiary library to `mlx` called `mlx-lm` for this very purpose. Technically speaking, if all you're interested in is this model inferenece, all you need is `mlx-lm` with a few short lines of code. We will be going a little bit deeper in this notebook for educational purposes.\n",
    "\n",
    "While these are the primary libraries that we'll be using, we also need to install a few more for dependency purposes. They will not necessarily be used directly.\n",
    "\n",
    "- `bitsandbytes`: `bitsandbytes` is another library also maintained by Hugging Face, and it is the standard for quantizing LLMs at a general level. We will quickly demonstrate how to make use of it on Mistral 7B, but given the focus of this notebook is more on MLX, we will not actually be making use of this Bits and Bytes quantized model. Additionally, we don't actually make direct use of this library if we use the `transformer` library. Instead, we simply make use of the `BitsAndBytesConfig`, which we will demonstrate in a later section.\n",
    "- `torch`: This is the Python client representing the popular ML framework PyTorch. While PyTorch remains a very popular library in the AI/ML community (including amongst Hugging Face users), we will not be using it in this notebook. The only reason we need it is as a dependency for the Bits & Bytes quantization.\n",
    "- `accelerate`: This Hugging Face library is designed to accelerate ML workflows, and our specific usage of it is as a dependency for Bits & Bytes to perform its quantization. (Note: We will actually not be able to effectively demonstrate this in this notebook, because as of this notebook's creation, `accelerate` does not support Apple Silicon hardware. I can still assure you the commented out code works fine in other environments!)\n",
    "\n",
    "\n",
    "You can install all these libraries by running the following command:\n",
    "\n",
    "`pip install transformers torch accelerate bitsandbytes mlx mlx-lm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing the necessary Python libraries\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from mlx_lm import load, generate, convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Mistral 7B from the Hugging Face Hub\n",
    "The first thing we will need to do is to load the [Mistral 7B model from the Hugging Face Hub](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2). If you are not familiar with how downloading LLMs from Hugging Face works, it's pretty easy to get up and going! Generally speaking, most LLMs can be loaded using `AutoModelForCausalLM` and `AutoTokenizer`. If the `Auto` prefix is throwing you off, it simply means that when you pass in a string representing the model name to one of these functions, it will automatically figure out what model architecture it needs to load on the backend to make your model work. So effectively, the `AutoModelForCausalLM` will actually turn into an architecture that specifically works for Mistral 7B.\n",
    "\n",
    "To save ourselves the headache of downloading the model artifacts every time, we will be saving them to a local directory. (Note: My `.gitignore` file is intentionally not pushing these files into GitHub.) This is because the model artifacts for Mistral 7B are roughly 28GB in size, so downloading them can take a little while depending on your internet speed. (I personally have gigabit wifi, and it takes about 10-15 minutes for me to download the artifacts.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting constant values to represent model name and directory\n",
    "MODEL_NAME = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "BASE_DIRECTORY = '../models'\n",
    "BNB_DIRECTORY = f'{BASE_DIRECTORY}/bnb'\n",
    "MLX_DIRECTORY = f'{BASE_DIRECTORY}/mlx'\n",
    "\n",
    "# Setting the full model directory path for each of our three model types (base, quantized with bits & bytes, quantized with MLX)\n",
    "model_directory = f'{BASE_DIRECTORY}/{MODEL_NAME}'\n",
    "bnb_model_directory = f'{BNB_DIRECTORY}/{MODEL_NAME}'\n",
    "mlx_model_directory = f'{MLX_DIRECTORY}/{MODEL_NAME}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model artifacts from disk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:47<00:00,  7.84s/it]\n"
     ]
    }
   ],
   "source": [
    "# Checking to see if the directory has already been created\n",
    "if os.path.exists(model_directory):\n",
    "\n",
    "    # Loading the tokenizer and model from local file\n",
    "    print('Loading the model artifacts from disk.')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_directory)\n",
    "\n",
    "else:\n",
    "\n",
    "    # Creating the new model directory\n",
    "    os.makedirs(model_directory)\n",
    "\n",
    "    # Downloading the tokenizer and model from Hugging Face\n",
    "    print('No local model found. Downloading artifacts from the Hugging Face Hub.')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    # Saving the tokenizer and model to model directory\n",
    "    print('Saving the model artifacts and tokenizer to disk.')\n",
    "    tokenizer.save_pretrained(save_directory = model_directory)\n",
    "    model.save_pretrained(save_directory = model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the model and tokenizer variables to save on memory (Comment out this cell if you don't want to remove these from RAM.)\n",
    "del tokenizer\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization with Bits & Bytes\n",
    "Now that we have downloaded our model artifacts, we can demonstrate how to generally quantize them using the Hugging Face standard Bits & Bytes library. You'll see just how easy this is to do! All we need to do is to set a configuration we would like to specify for how for Bits & Bytes to perform the quantization.\n",
    "\n",
    "Unfortunately, this code actually is not compatible with Apple Silicon, so I will leave it commented out. But I can confirm this will work in environments like an AWS SageMaker notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading the Bits & Bytes configuration for 4-bit quantization\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit = True,\n",
    "#     bnb_4bit_quant_type = \"nf4\",  # Use NF4 data type for weights from normal distribution\n",
    "#     bnb_4bit_use_double_quant = True,  # Use nested quantization\n",
    "#     bnb_4bit_compute_dtype = torch.bfloat16  # Use bfloat16 for faster computation\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Checking to see if the directory has already been created\n",
    "# if os.path.exists(bnb_model_directory):\n",
    "\n",
    "#     # Loading the tokenizer and model from local file\n",
    "#     print('Loading the BNB-quantized model artifacts from disk.')\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(bnb_model_directory)\n",
    "#     model = AutoModelForCausalLM.from_pretrained(bnb_model_directory)\n",
    "\n",
    "# else:\n",
    "\n",
    "#     # Creating the new model directory\n",
    "#     os.makedirs(bnb_model_directory)\n",
    "\n",
    "#     # Downloading the tokenizer and model from Hugging Face\n",
    "#     print('No local BNB model found. Quantizing artifacts using Bits & Bytes.')\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
    "#     model = AutoModelForCausalLM.from_pretrained(model_directory, quantization_config = bnb_config)\n",
    "\n",
    "#     # Saving the tokenizer and model to model directory\n",
    "#     print('Saving the BNB-quantized model artifacts and tokenizer to disk.')\n",
    "#     tokenizer.save_pretrained(save_directory = bnb_model_directory)\n",
    "#     model.save_pretrained(save_directory = bnb_model_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization with MLX\n",
    "Now that we have demonstrated how to perform quantization with Bits & Bytes, let's move into quantization with MLX. While it is certainly possible to do this with the base `mlx` library, Apple has made it very easy on us by enabling this functionality within the `mlx-lm` library. Please be aware this quantization is not supported for all models on the Hugging Face Hub. For a list of models that you can effectively quantize with MLX, [please visit this page](https://github.com/ml-explore/mlx-examples/tree/main/llms#supported-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the MLX-quantized model artifacts from disk.\n"
     ]
    }
   ],
   "source": [
    "# Checking to see if the directory has already been created\n",
    "if os.path.exists(mlx_model_directory):\n",
    "\n",
    "    # Loading the tokenizer and model from local file\n",
    "    print('Loading the MLX-quantized model artifacts from disk.')\n",
    "    model, tokenizer = load(mlx_model_directory)\n",
    "\n",
    "else:\n",
    "\n",
    "    # Creating the new model directory\n",
    "    os.makedirs(mlx_model_directory)\n",
    "\n",
    "    # Loading the tokenizer from disk\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
    "\n",
    "    # Saving the tokenizer and model to model directory\n",
    "    print('Saving the MLX-quantized model artifacts and tokenizer to disk.')\n",
    "    tokenizer.save_pretrained(save_directory = mlx_model_directory)\n",
    "    convert(hf_path = model_directory,\n",
    "            mlx_path = mlx_model_directory,\n",
    "            quantize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Use of the Quantized MLX Model\n",
    "Now that we have successfully quantized Mistral 7B, let's demonstrate now how we can effectively make use of it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Mesa have dewicious chwocow chip cowookies fow you! Mesa give you step by step instructions, but mesa warn you, dis will be a wowising journey to de wowest cookie in the galaxy!\n",
      "\n",
      "Ingredients you need:\n",
      "- 2 1/4 cups all-purpose flour\n",
      "- 1/2 cup unsalted butter, softened\n",
      "- 1/2 cup shortening\n",
      "- 1 cup granulated sugar\n",
      "- 1 cup packed brown sugar\n",
      "- 2 eggs\n",
      "- 2 teaspoons vanilla extract\n",
      "- 3 cups quick-cooking oats\n",
      "- 1 teaspoon baking soda\n",
      "- 1/2 teaspoon baking powder\n",
      "- 1/2 teaspoon salt\n",
      "- 2 cups semisweet chocolate chips\n",
      "- 1 cup chopped walnuts (optional)\n",
      "\n",
      "Mesa begin! Preheat oven to 350 degrees F (175 degrees C). Mesa cream butter, shortening, granulated sugar, and brown sugar until light and fluffy. Mesa add eggs one at a time, beating well after each addition. Mesa stir in vanilla extract.\n",
      "\n",
      "Mesa combine flour, baking soda, baking powder, and salt; gradually add to butter mixture and mix well. Mesa add oats, chocolate chips, and nuts (if desired).\n",
      "\n",
      "Mesa drop by rounded tablespoonfuls onto ungreased baking sheets. Mesa bake for 10 to 12 minutes or until edges are lightly toasted.\n",
      "\n",
      "Mesa let cookies cool on baking sheets for 5 minutes before transferring to wire racks to cool completely.\n",
      "\n",
      "Mesa serve with a glass of galactic milk and enjoy the wowest chocolate chip cookies in the galaxy! Mesa hope you find dis recipe easy to follow, and mesa wish you a happy baking experience! May the Force be with you!\n"
     ]
    }
   ],
   "source": [
    "# Setting the prompt we would like to ask of the quantized model\n",
    "prompt = 'Give me a recipe for delicious chocolate chip cookies. Please give the response in the tone of Jar Jar Binks.'\n",
    "\n",
    "# Producing the response (completion) with the quantized model\n",
    "response = generate(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    prompt = prompt,\n",
    "    max_tokens = 1000\n",
    ")\n",
    "\n",
    "# Printing the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
