{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Conversation History Management\n",
    "In the [LangChain Expression Language (LCEL)](./lcel.ipynb), we covered LCEL at a high level, demonstrating specifically how to chain a prompt engineered chat prompt with an LLM, namely MLX. What I failed to demonstrate in that notebook was how to think about memory (aka chat conversation management), and to be honest, I underestimated how \"involved\" of a thing this got to be! ðŸ˜… To be clear, what we will be covering in this notebook is less of a technical concern and more of a business logic concern.\n",
    "\n",
    "While LangChain offers many mechanisms for handling chat conversations (aka memory) correctly, I found some of the higher level ones to not be satisfactory for our purposes. Specifically, since I want us to adhere to a fixed schema, the high level abstraction objects provided by LangChain simply don't operate in the ideal way in which we need them to. No worries! We can still work around this without having to abandon LangChain. We're just going to need to do some special stuff throughout this notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Level Flow\n",
    "Before we get into the code itself, let's talk about how we want to think about the flow. For simplicity's sake, we are going to be ultimately saving this chat history as a JSON file. This JSON file should look like the schema that we've defined in the file `data/schema.json`.\n",
    "\n",
    "Let's say that the user is loading the MLX Gradio UI interface, either for the first time ever or as a returning user. Here is the flow of how we should be thinking about our data:\n",
    "\n",
    "1. **Loading the conversation history from file**: Just as it sounds, we will want to load the conversation history from file so that the user can interact with their historical conversations if they would like. Now, it's possible that this is the user's first time interacting with the chatbot, so it may be that we need to create this file from scratch!\n",
    "2. **Setting a new conversation ID**: Regardless if the user is new or returning, we are going to make the assumption that the user will want to begin with a new conversation. This means that we will need to instantiate a new conversation ID so that we can keep appending new conversation interactions to that same conversation thread.\n",
    "3. **Managing conversation back-and-forth**: As the conversation proceeds, we will want to continually update our conversation schema with any new human and AI interactions. This will include also autosaving them to file for the user's convenience.\n",
    "4. **Starting a new conversation / loading an existing conversation**: At any point, the user may want to pivot from their current conversation to either a new conversation or to continue another historical conversation loaded from our file as part of step 1. If this is the case, we will need to ensure that our backend system is referencing the correct conversation interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "In this section, we'll do all our usual set ups. We'll also set up the LangChain prompt structure and model. All these are things we've already explored in other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary Python libraries\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_community.llms.mlx_pipeline import MLXPipeline\n",
    "from langchain_community.chat_models.mlx import ChatMLX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a default system prompt\n",
    "DEFAULT_SYSTEM_PROMPT = 'You are a helpful assistant.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the Chat prompt template\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template = DEFAULT_SYSTEM_PROMPT)\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(template = \"{input}\")\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting constant values to represent model name and directory\n",
    "MODEL_NAME = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "BASE_DIRECTORY = '../models'\n",
    "MLX_DIRECTORY = f'{BASE_DIRECTORY}/mlx'\n",
    "mlx_model_directory = f'{MLX_DIRECTORY}/{MODEL_NAME}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Setting up the LangChain MLX LLM\n",
    "llm = MLXPipeline.from_model_id(\n",
    "    model_id = mlx_model_directory,\n",
    "    pipeline_kwargs = {\n",
    "        'temp': 0.7,\n",
    "        'max_tokens': 1000\n",
    "    }\n",
    ")\n",
    "\n",
    "# Setting up the LangChain MLX Chat Model with the LLM above\n",
    "chat_model = ChatMLX(llm = llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the Conversation History Schema\n",
    "As mentioned before, we are going to be emulating the structure of the schema as defined in `data/schema.json`. In this notebook, we are going to pretend as if the user is a brand new user, so we will need to set up the conversation history schema from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c0393de1-2a2f-4e6a-80d0-cee64fcc6b60'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating a current conversation ID\n",
    "current_conversation_id = str(uuid.uuid4())\n",
    "current_conversation_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': 'default_username',\n",
       " 'conversation_history': [{'conversation_id': '6c1bdf13-f669-45d4-a2bd-e2ac6cb19e79',\n",
       "   'summary_title': '',\n",
       "   'system_prompt': 'You are a helpful assistant.',\n",
       "   'conversation': []}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the base conversation history schema per a single user\n",
    "BASE_CONVERSATION_HISTORY_SCHEMA = {\n",
    "    'user_id': 'default_username',\n",
    "    'conversation_history': [\n",
    "        {\n",
    "            'conversation_id': current_conversation_id,\n",
    "            'summary_title': '',\n",
    "            'system_prompt': DEFAULT_SYSTEM_PROMPT,\n",
    "            'conversation': []\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "BASE_CONVERSATION_HISTORY_SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': 'default_username',\n",
       " 'conversation_history': [{'conversation_id': '6c1bdf13-f669-45d4-a2bd-e2ac6cb19e79',\n",
       "   'summary_title': '',\n",
       "   'system_prompt': 'You are a helpful assistant.',\n",
       "   'conversation': []}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_CONVERSATION_HISTORY_SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
