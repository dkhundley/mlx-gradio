{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Conversation History Management\n",
    "In the [LangChain Expression Language (LCEL)](./lcel.ipynb), we covered LCEL at a high level, demonstrating specifically how to chain a prompt engineered chat prompt with an LLM, namely MLX. What I failed to demonstrate in that notebook was how to think about memory (aka chat conversation management), and to be honest, I underestimated how \"involved\" of a thing this got to be! ðŸ˜… To be clear, what we will be covering in this notebook is less of a technical concern and more of a business logic concern.\n",
    "\n",
    "While LangChain offers many mechanisms for handling chat conversations (aka memory) correctly, I found some of the higher level ones to not be satisfactory for our purposes. Specifically, since I want us to adhere to a fixed schema, the high level abstraction objects provided by LangChain simply don't operate in the ideal way in which we need them to. No worries! We can still work around this without having to abandon LangChain. We're just going to need to do some special stuff throughout this notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Level Flow\n",
    "Before we get into the code itself, let's talk about how we want to think about the flow. For simplicity's sake, we are going to be ultimately saving this chat history as a JSON file. This JSON file should look like the schema that we've defined in the file `data/schema.json`.\n",
    "\n",
    "Let's say that the user is loading the MLX Gradio UI interface, either for the first time ever or as a returning user. Here is the flow of how we should be thinking about our data:\n",
    "\n",
    "1. **Loading the conversation history from file**: Just as it sounds, we will want to load the conversation history from file so that the user can interact with their historical conversations if they would like. Now, it's possible that this is the user's first time interacting with the chatbot, so it may be that we need to create this file from scratch!\n",
    "2. **Setting a new conversation ID**: Regardless if the user is new or returning, we are going to make the assumption that the user will want to begin with a new conversation. This means that we will need to instantiate a new conversation ID so that we can keep appending new conversation interactions to that same conversation thread.\n",
    "3. **Managing conversation back-and-forth**: As the conversation proceeds, we will want to continually update our conversation schema with any new human and AI interactions. This will include also autosaving them to file for the user's convenience.\n",
    "4. **Starting a new conversation / loading an existing conversation**: At any point, the user may want to pivot from their current conversation to either a new conversation or to continue another historical conversation loaded from our file as part of step 1. If this is the case, we will need to ensure that our backend system is referencing the correct conversation interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "In this section, we'll do all our usual set ups. We'll also set up the LangChain MLX model using the new ChatMLX implementation. All these are things we've already explored in other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary Python libraries\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.llms.mlx_pipeline import MLXPipeline\n",
    "from langchain_community.chat_models.mlx import ChatMLX\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_community.chat_message_histories.in_memory import ChatMessageHistory\n",
    "from langchain_core.messages.human import HumanMessage\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "from langchain_core.messages.system import SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a default system prompt\n",
    "DEFAULT_SYSTEM_PROMPT = 'You are a helpful assistant.'\n",
    "\n",
    "class MLXModelParameters():\n",
    "\n",
    "    def __init__(self, temp = 0.7, max_tokens = 1000, system_prompt = DEFAULT_SYSTEM_PROMPT):\n",
    "        self.temp = temp\n",
    "        self.max_tokens = max_tokens\n",
    "        self.system_prompt = system_prompt\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'Temperature: {self.temp}\\nMax Tokens: {self.max_tokens}\\nSystem Prompt: {self.system_prompt}'\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'Temperature: {self.temp}\\nMax Tokens: {self.max_tokens}\\System Prompt: {self.system_prompt}'\n",
    "\n",
    "    def update_temp(self, new_temp):\n",
    "        self.temp = new_temp\n",
    "\n",
    "    def update_max_tokens(self, new_max_tokens):\n",
    "        self.max_tokens = new_max_tokens\n",
    "\n",
    "    def update_system_prompt(self, new_system_prompt):\n",
    "        self.system_prompt = new_system_prompt\n",
    "\n",
    "    def to_json(self):\n",
    "        return { 'temp': self.temp, 'max_tokens': self.max_tokens }\n",
    "    \n",
    "mlx_model_parameters = MLXModelParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a list of models that we'll need to check against\n",
    "NO_SYSTEM_MODEL_PROVIDERS = ['mistralai', 'meta-llama']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting constant values to represent model name and directory\n",
    "MODEL_NAME = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "BASE_DIRECTORY = '../models'\n",
    "MLX_DIRECTORY = f'{BASE_DIRECTORY}/mlx'\n",
    "mlx_model_directory = f'{MLX_DIRECTORY}/{MODEL_NAME}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Setting up the LangChain MLX LLM\n",
    "llm = MLXPipeline.from_model_id(\n",
    "    model_id = mlx_model_directory,\n",
    "    pipeline_kwargs = {\n",
    "        'temp': mlx_model_parameters.temp,\n",
    "        'max_tokens': mlx_model_parameters.max_tokens,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Setting up the LangChain MLX Chat Model with the LLM above\n",
    "chat_model = ChatMLX(llm = llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the LangChain pipeline\n",
    "In order to use LangChain's preferred implementation of memory management, we're first going to need to establish our LangChain pipeline. We've done similar things to this in other notebooks, but with this particular implementation, we are going to make a specific adjustment. Namely, since we are now going to make use of the LangChain Community implementation of MLX, we are going to need to manually add our own metadata. To seamlessly do this, we are going to make use of LCEL's **RunnableLambda**, which essentially allows us to define our own custom function.\n",
    "\n",
    "Also note that when we set up our chat prompt, we are going to need to slide in an extra entry referred to as **MessagesPlaceholder**. As the name implies, that will serve as a placeholder so that we can keep passing the history back through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the Chat prompt template\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(template = \"{input}\")\n",
    "chat_prompt = ChatPromptTemplate.from_messages(messages = [\n",
    "    MessagesPlaceholder(variable_name = 'history'),\n",
    "    human_message_prompt\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_ai_response_metadata(ai_message):\n",
    "    '''\n",
    "    Updates the metadata on the AI response\n",
    "\n",
    "    Inputs:\n",
    "        - ai_message (LangChain AIMessage): The AI message produced by the model\n",
    "\n",
    "    Returns:\n",
    "        - ai_message (LangChain AIMessage): The AI message produced by the model, except now with the appropriate metadata intact\n",
    "    '''\n",
    "\n",
    "    # Referencing global variables\n",
    "    global mlx_model_parameters\n",
    "    global MODEL_NAME\n",
    "\n",
    "    # Creating a dictionary of the metadata that we will be adding to the AI message\n",
    "    metadata = {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'timestamp': str(pd.Timestamp.utcnow()),\n",
    "        'like_data': None,\n",
    "        'hyperparameters': mlx_model_parameters.to_json()\n",
    "    }\n",
    "\n",
    "    # Applying the metadata to the AI response\n",
    "    ai_message.response_metadata = metadata\n",
    "\n",
    "    return ai_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_for_llama_and_mistral(chat_history):\n",
    "    '''\n",
    "    Corrects for the issue where Llama / Mistral models are unable to accept LangChain system messages\n",
    "\n",
    "    Inputs:\n",
    "        - chat_history (LangChain ChatPromptValue): The current chat history with no alterations\n",
    "\n",
    "    Returns:\n",
    "        - chat_history (LangChain ChatPromptValue): The chat history with alterations (if needed)\n",
    "    '''\n",
    "    # Referencing global variables\n",
    "    global MODEL_NAME\n",
    "    global NO_SYSTEM_MODEL_PROVIDERS\n",
    "\n",
    "    # Checking if the correction needs to be made if the model is Llama or Mistral\n",
    "    if MODEL_NAME.split('/')[0] in NO_SYSTEM_MODEL_PROVIDERS:\n",
    "\n",
    "        # Popping the system messages out of the queue of messages\n",
    "        system_message = chat_history.messages.pop(0)\n",
    "\n",
    "        # Getting the first human message\n",
    "        first_human_message = chat_history.messages[0].content\n",
    "\n",
    "        # Joining the system message with the first human message as a new human message\n",
    "        new_human_message = f'{system_message.content}\\n\\n{first_human_message}'\n",
    "\n",
    "        # Setting this new human message as the content in the first human message in the chat history\n",
    "        chat_history.messages[0].content = new_human_message\n",
    "\n",
    "    return chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the inference chain by chaining together the chat prompt, chat model, and custom function to update metadata\n",
    "inference_chain = chat_prompt | RunnableLambda(correct_for_llama_and_mistral) | chat_model | RunnableLambda(update_ai_response_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest city in Illinois is Chicago. While Springfield is the state capital, Chicago is the most populous city in Illinois.\n",
      "{'model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'timestamp': '2024-04-20 19:19:06.012664+00:00', 'like_data': None, 'hyperparameters': {'temp': 0.7, 'max_tokens': 1000}}\n"
     ]
    }
   ],
   "source": [
    "# Testing the inference chain with a fake chat history\n",
    "fake_chat_history = ChatMessageHistory(messages = [\n",
    "    SystemMessage(content = 'You are a helpful assistant.'),\n",
    "    HumanMessage(content = 'What is the capital of Illinois?'),\n",
    "    AIMessage(content = 'The capital of Illinois is Springfield.')\n",
    "])\n",
    "\n",
    "# Generating the test response with the fake chat history\n",
    "test_response = inference_chain.invoke({\n",
    "    'history': fake_chat_history.messages,\n",
    "    'input': 'What is the largest city in that state?'\n",
    "})\n",
    "\n",
    "print(test_response.content)\n",
    "print(test_response.response_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the Conversation History Schema\n",
    "As mentioned before, we are going to be emulating the structure of the schema as defined in `data/schema.json`. In this notebook, we are going to pretend as if the user is a brand new user, so we will need to set up the conversation history schema from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1ab6d990-db3f-4e47-81d2-31fe2a76d1f1'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating a current conversation ID\n",
    "current_conversation_id = str(uuid.uuid4())\n",
    "current_conversation_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': 'default_username',\n",
       " 'conversation_history': [{'conversation_id': '1ab6d990-db3f-4e47-81d2-31fe2a76d1f1',\n",
       "   'summary_title': '',\n",
       "   'system_prompt': 'You are a helpful assistant.',\n",
       "   'conversation': []}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the base conversation history schema per a single user\n",
    "BASE_CONVERSATION_HISTORY_SCHEMA = {\n",
    "    'user_id': 'default_username',\n",
    "    'conversation_history': [\n",
    "        {\n",
    "            'conversation_id': current_conversation_id,\n",
    "            'summary_title': '',\n",
    "            'system_prompt': DEFAULT_SYSTEM_PROMPT,\n",
    "            'conversation': []\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "BASE_CONVERSATION_HISTORY_SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': 'default_username',\n",
       " 'conversation_history': [{'conversation_id': '1ab6d990-db3f-4e47-81d2-31fe2a76d1f1',\n",
       "   'summary_title': '',\n",
       "   'system_prompt': 'You are a helpful assistant.',\n",
       "   'conversation': []}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_CONVERSATION_HISTORY_SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories.in_memory import ChatMessageHistory\n",
    "from langchain_core.messages.system import SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = ChatMessageHistory(\n",
    "    messages = [\n",
    "        SystemMessage(content = DEFAULT_SYSTEM_PROMPT)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages.ai import AIMessage\n",
    "\n",
    "ai_message = AIMessage(content = 'The capital of Illinois is Springfield')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_message.response_metadata = {'temp': 0.7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_model.invoke('What is the capital of Illinois?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages.human import HumanMessage\n",
    "chat_history = ChatMessageHistory()\n",
    "chat_history.add_messages(messages = [\n",
    "    HumanMessage(content = 'What is the capital of Illinois?'),\n",
    "    AIMessage(content = 'The capital of Illinois is Springfield.')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseModel.schema_json of <class 'langchain_community.chat_message_histories.in_memory.ChatMessageHistory'>>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChatMessageHistory.schema_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
