{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLX LangChain Chatbot\n",
    "In this notebook, we will build a LangChain-based chatbot from end-to-end using MLX as the LLM-hosting service. This notebook is more or less an amalgamation of concepts covered from other sources, so we will tend to gloss over certain things here but will still provide links to those other resources for those who want to dive deeper.\n",
    "\n",
    "The headings of markdown cells below will largely correlate to the same blog post on Medium. As such, minimal information will be provided beyond the code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "Throughout this notebook, we will largely be making use of only MLX and LangChain, along with other standand Python functions. In order to make use of these libraries, you will need to install them appropriately. For example, if you use `pip`, you will want to run the following command:\n",
    "\n",
    "`pip install mlx mlx-lm langchain langchain-community langchain-core`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing the necessary Python libraries\n",
    "from mlx_lm import load as mlx_load\n",
    "from mlx_lm import generate as mlx_generate\n",
    "from langchain_community.llms.mlx_pipeline import MLXPipeline\n",
    "from langchain_community.chat_models.mlx import ChatMLX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting constant values to represent model name and directory\n",
    "MODEL_NAME = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "BASE_DIRECTORY = '../models'\n",
    "MLX_DIRECTORY = f'{BASE_DIRECTORY}/mlx'\n",
    "mlx_model_directory = f'{MLX_DIRECTORY}/{MODEL_NAME}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Quick Intro to MLX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "In the flow of code,\n",
      "Notebook pages gently turn,\n",
      "Data's story unfolds.\n"
     ]
    }
   ],
   "source": [
    "# Loading the MLX quantized Mistral 7B model from file\n",
    "mlx_model, mlx_tokenizer = mlx_load(mlx_model_directory)\n",
    "\n",
    "# Producing the response (completion) with the MLX model\n",
    "response = mlx_generate(\n",
    "    model = mlx_model,\n",
    "    tokenizer = mlx_tokenizer,\n",
    "    prompt = 'Write me a haiku about Jupyter notebooks.',\n",
    "    max_tokens = 1000\n",
    ")\n",
    "\n",
    "# Printing the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
