{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral on MLX\n",
    "In this notebook, we demonstrate how to run the open source model **Mistral** on your Apple Silicon computer. This notebook will largely be based on [the example in Apple's GitHub repository](https://github.com/ml-explore/mlx-examples/tree/main/llms/mistral)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing the necessary Python libraries\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting constant values to represent model name and directory\n",
    "MODEL_NAME = 'mistralai/Mixtral-8x7B-Instruct-v0.1'\n",
    "BASE_DIRECTORY = '../models/'\n",
    "\n",
    "# Setting the full model directory path\n",
    "model_directory = f'{BASE_DIRECTORY}{MODEL_NAME}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 19/19 [20:59<00:00, 66.27s/it]\n",
      "Loading checkpoint shards:  21%|██        | 4/19 [01:09<04:48, 19.23s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Checking to see if the directory has already been created\n",
    "if os.path.exists(model_directory):\n",
    "\n",
    "    # Loading the tokenizer and model from local file\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_directory)\n",
    "\n",
    "else:\n",
    "\n",
    "    # Creating the new model directory\n",
    "    os.makedirs(model_directory)\n",
    "\n",
    "    # Downloading the tokenizer and model from HuggingFace\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    # Saving the tokenizer and model to model directory\n",
    "    tokenizer.save_pretrained(save_directory = model_directory)\n",
    "    model.save_pretrained(save_directory = model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tokenizer.encode('Hello world!', return_tensors = 'pt')\n",
    "# outputs = model.generate(inputs, max_length = 5)\n",
    "# tokenizer.decode(outputs[0], skip_special_tokens = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path(MODEL_NAME)\n",
    "model_path = Path(\n",
    "    snapshot_download(\n",
    "        repo_id = MODEL_NAME,\n",
    "        revision = None,\n",
    "        allow_patterns = [\n",
    "            '*.json',\n",
    "            '*.safetensors',\n",
    "            '*.py',\n",
    "            'tokenizer.model',\n",
    "            '*.tiktoken'\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: What is the capital of Illinois? Answer using the tone of Jar Jar Binks.\n",
      "\n",
      "\n",
      "Meesa talkin' 'bout the capital of Illinois, yessiree! It's Springfield, meesa say! Yessiree, Springfield it is! Binks know, Binks very smart!\n",
      "==========\n",
      "Prompt: 45.056 tokens-per-sec\n",
      "Generation: 27.483 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load('../models/mlx_model')\n",
    "response = generate(model, tokenizer, prompt = 'What is the capital of Illinois? Answer using the tone of Jar Jar Binks.', verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Fetching 8 files: 100%|██████████| 8/8 [01:20<00:00, 10.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: hello\n",
      ",\n",
      "\n",
      "I am writing to you to inquire about the possibility of purchasing a domain name through your company.\n",
      "\n",
      "I am interested in purchasing the domain name [domain name] and would like to know if you are able to provide me with a quote for the purchase.\n",
      "\n",
      "Please let me know if you require any further information from me, such as the desired length of the lease or the purpose of the domain name.\n",
      "\n",
      "Thank you for your time and consideration.\n",
      "\n",
      "Sincerely,\n",
      "[Your Name]\n",
      "==========\n",
      "Prompt: 1.547 tokens-per-sec\n",
      "Generation: 21.364 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model, tokenizer = load(\"mlx-community/quantized-gemma-7b-it\")\n",
    "response = generate(model, tokenizer, prompt=\"hello\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Mistral' from 'mistralai' (/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/mistralai/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmistralai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Mistral, ModelArgs\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Mistral' from 'mistralai' (/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/mistralai/__init__.py)"
     ]
    }
   ],
   "source": [
    "from mistralai import Mistral, ModelArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
