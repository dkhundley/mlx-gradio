{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Chat History Management\n",
    "In the [LangChain Expression Language (LCEL)](./lcel.ipynb), we covered LCEL at a high level, demonstrating specifically how to chain a prompt engineered chat prompt with an LLM, namely MLX. What I failed to demonstrate in that notebook was how to think about memory (aka chat conversation management), and to be honest, I underestimated how \"involved\" of a thing this got to be! ðŸ˜… To be clear, what we will be covering in this notebook is less of a technical concern and more of a business logic concern.\n",
    "\n",
    "While LangChain offers many mechanisms for handling chat conversations (aka memory) correctly, I found some of the higher level ones to not be satisfactory for our purposes. Specifically, since I want us to adhere to a fixed schema, the high level abstraction objects provided by LangChain simply don't operate in the ideal way in which we need them to. No worries! We can still work around this without having to abandon LangChain. We're just going to need to do some special stuff throughout this notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Level Flow\n",
    "Before we get into the code itself, let's talk about how we want to think about the flow. For simplicity's sake, we are going to be ultimately saving this chat history as a JSON file. This JSON file should look like the schema that we've defined in the file `data/schema.json`.\n",
    "\n",
    "Let's say that the user is loading the MLX Gradio UI interface, either for the first time ever or as a returning user. Here is the flow of how we should be thinking about our data:\n",
    "\n",
    "1. **Loading the conversation history from file**: Just as it sounds, we will want to load the conversation history from file so that the user can interact with their historical conversations if they would like. Now, it's possible that this is the user's first time interacting with the chatbot, so it may be that we need to create this file from scratch!\n",
    "2. **Setting a new conversation ID**: Regardless if the user is new or returning, we are going to make the assumption that the user will want to begin with a new conversation. This means that we will need to instantiate a new conversation ID so that we can keep appending new conversation interactions to that same conversation thread.\n",
    "3. **Managing conversation back-and-forth**: As the conversation proceeds, we will want to continually update our conversation schema with any new human and AI interactions. This will include also autosaving them to file for the user's convenience.\n",
    "4. **Starting a new conversation / loading an existing conversation**: At any point, the user may want to pivot from their current conversation to either a new conversation or to continue another historical conversation loaded from our file as part of step 1. If this is the case, we will need to ensure that our backend system is referencing the correct conversation interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "In this section, we'll do all our usual set ups. We'll also set up the LangChain MLX model using the new ChatMLX implementation. All these are things we've already explored in other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary Python libraries\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.llms.mlx_pipeline import MLXPipeline\n",
    "from langchain_community.chat_models.mlx import ChatMLX\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_community.chat_message_histories.in_memory import ChatMessageHistory\n",
    "from langchain_core.messages.human import HumanMessage\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "from langchain_core.messages.system import SystemMessage\n",
    "from langchain_core.prompt_values import ChatPromptValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a default system prompt\n",
    "DEFAULT_SYSTEM_PROMPT = 'You are a helpful assistant.'\n",
    "\n",
    "class MLXModelParameters():\n",
    "\n",
    "    def __init__(self, temp = 0.7, max_tokens = 1000, system_prompt = DEFAULT_SYSTEM_PROMPT):\n",
    "        self.temp = temp\n",
    "        self.max_tokens = max_tokens\n",
    "        self.system_prompt = system_prompt\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'Temperature: {self.temp}\\nMax Tokens: {self.max_tokens}\\nSystem Prompt: {self.system_prompt}'\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'Temperature: {self.temp}\\nMax Tokens: {self.max_tokens}\\System Prompt: {self.system_prompt}'\n",
    "\n",
    "    def update_temp(self, new_temp):\n",
    "        self.temp = new_temp\n",
    "\n",
    "    def update_max_tokens(self, new_max_tokens):\n",
    "        self.max_tokens = new_max_tokens\n",
    "\n",
    "    def update_system_prompt(self, new_system_prompt):\n",
    "        self.system_prompt = new_system_prompt\n",
    "\n",
    "    def to_json(self):\n",
    "        return { 'temp': self.temp, 'max_tokens': self.max_tokens }\n",
    "    \n",
    "mlx_model_parameters = MLXModelParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a starter conversation history object\n",
    "STARTER_CONVERSATION_HISTORY = ChatMessageHistory(messages = [ SystemMessage(content = 'You are a helpful assistant.') ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a list of models that we'll need to check against\n",
    "NO_SYSTEM_MODEL_PROVIDERS = ['mistralai', 'meta-llama']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting constant values to represent model name and directory\n",
    "MODEL_NAME = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "BASE_DIRECTORY = '../models'\n",
    "MLX_DIRECTORY = f'{BASE_DIRECTORY}/mlx'\n",
    "mlx_model_directory = f'{MLX_DIRECTORY}/{MODEL_NAME}'\n",
    "\n",
    "# Setting a constant value to represent where to place the chat history data\n",
    "CHAT_HISTORY_DIRECTORY = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Setting up the LangChain MLX LLM\n",
    "llm = MLXPipeline.from_model_id(\n",
    "    model_id = mlx_model_directory,\n",
    "    pipeline_kwargs = {\n",
    "        'temp': mlx_model_parameters.temp,\n",
    "        'max_tokens': mlx_model_parameters.max_tokens,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Setting up the LangChain MLX Chat Model with the LLM above\n",
    "chat_model = ChatMLX(llm = llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the LangChain pipeline\n",
    "In order to use LangChain's preferred implementation of memory management, we're first going to need to establish our LangChain pipeline. We've done similar things to this in other notebooks, but with this particular implementation, we are going to make a specific adjustment. Namely, since we are now going to make use of the LangChain Community implementation of MLX, we are going to need to manually add our own metadata. To seamlessly do this, we are going to make use of LCEL's **RunnableLambda**, which essentially allows us to define our own custom function.\n",
    "\n",
    "Also note that when we set up our chat prompt, we are going to need to slide in an extra entry referred to as **MessagesPlaceholder**. As the name implies, that will serve as a placeholder so that we can keep passing the history back through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the Chat prompt template\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(template = \"{input}\")\n",
    "chat_prompt = ChatPromptTemplate.from_messages(messages = [\n",
    "    MessagesPlaceholder(variable_name = 'history'),\n",
    "    human_message_prompt\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_for_no_system_models(chat_history):\n",
    "    '''\n",
    "    Precorrects for the issue where certain models (e.g. Llama, Mistral) are unable to accept for system messages\n",
    "\n",
    "    Inputs:\n",
    "        - chat_history (LangChain ChatPromptValue): The current chat history with no alterations\n",
    "\n",
    "    Returns:\n",
    "        - chat_history (LangChain ChatPromptValue): The new chat history with alterations (if needed)\n",
    "    '''\n",
    "    # Referencing global variables\n",
    "    global MODEL_NAME\n",
    "    global NO_SYSTEM_MODEL_PROVIDERS\n",
    "\n",
    "\n",
    "    # Checking if the correction needs to be made if the model is Llama or Mistral\n",
    "    if MODEL_NAME.split('/')[0] in NO_SYSTEM_MODEL_PROVIDERS:\n",
    "\n",
    "        # Getting the system message content\n",
    "        system_message_content = chat_history.messages[0].content\n",
    "\n",
    "        # Replacing the System Message with a Human Message\n",
    "        chat_history.messages[0] = HumanMessage(content = system_message_content)\n",
    "\n",
    "        # Adds a dummy AI Message\n",
    "        chat_history.messages.insert(1, AIMessage(content = ''))\n",
    "\n",
    "    return chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_ai_response_metadata(ai_message):\n",
    "    '''\n",
    "    Updates the metadata on the AI response\n",
    "\n",
    "    Inputs:\n",
    "        - ai_message (LangChain AIMessage): The AI message produced by the model\n",
    "\n",
    "    Returns:\n",
    "        - ai_message (LangChain AIMessage): The AI message produced by the model, except now with the appropriate metadata intact\n",
    "    '''\n",
    "\n",
    "    # Referencing global variables\n",
    "    global mlx_model_parameters\n",
    "    global MODEL_NAME\n",
    "\n",
    "    # Creating a dictionary of the metadata that we will be adding to the AI message\n",
    "    metadata = {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'timestamp': str(pd.Timestamp.utcnow()),\n",
    "        'like_data': None,\n",
    "        'hyperparameters': mlx_model_parameters.to_json()\n",
    "    }\n",
    "\n",
    "    # Applying the metadata to the AI response\n",
    "    ai_message.response_metadata = metadata\n",
    "\n",
    "    return ai_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_chat_history(ai_message):\n",
    "    '''\n",
    "    Adds messages to chat history\n",
    "\n",
    "    Inputs:\n",
    "        - ai_message (LangChain AIMessage): The AI message produced by the AI model\n",
    "\n",
    "    Returns:\n",
    "        - ai_message (LangChain AIMessage): Passing the AI message through to the end of the inference pipeline\n",
    "    '''\n",
    "\n",
    "    # Referencing global variables\n",
    "    global chat_history\n",
    "    global prompt_text\n",
    "\n",
    "    # Adding the AI message to the chat history\n",
    "    chat_history.add_messages(\n",
    "        messages = [\n",
    "            HumanMessage(content = prompt_text),\n",
    "            ai_message\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return ai_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the inference chain by chaining together the chat prompt, chat model, and custom function to update metadata\n",
    "inference_chain = chat_prompt | RunnableLambda(correct_for_no_system_models) | chat_model | RunnableLambda(update_ai_response_metadata) | RunnableLambda(add_to_chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First prompt: \"What is the capital of Illinois?\n",
      "AI Response: The capital city of Illinois is Springfield. It's located in the central part of the state and is the county seat of Sangamon County. Springfield is known for its rich history, including being the site of Abraham Lincoln's presidential library and his former home, which is now a National Historic Site.\"\n",
      "AI Response Metadata: {'model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'timestamp': '2024-04-20 21:23:52.736214+00:00', 'like_data': None, 'hyperparameters': {'temp': 0.7, 'max_tokens': 1000}}\n",
      "Current chat history:\n",
      "[SystemMessage(content='You are a helpful assistant.'), HumanMessage(content='What is the capital of Illinois?'), AIMessage(content=\"The capital city of Illinois is Springfield. It's located in the central part of the state and is the county seat of Sangamon County. Springfield is known for its rich history, including being the site of Abraham Lincoln's presidential library and his former home, which is now a National Historic Site.\", response_metadata={'model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'timestamp': '2024-04-20 21:23:52.736214+00:00', 'like_data': None, 'hyperparameters': {'temp': 0.7, 'max_tokens': 1000}}, id='run-612361f8-f940-42b4-b4a5-908c9a415c28-0')]\n",
      "\n",
      "-------------\n",
      "\n",
      "Second prompt: \"What is the largest city in that state?\"\n",
      "AI Response: The largest city in Illinois is Chicago. Chicago is located in the northeastern part of the state and is the third most populous city in the United States. It's known for its iconic skyline, major industries, cultural institutions, and its role as a global hub for commerce, transportation, and technology. Chicago is also famous for its architecture, museums, and its food scene, particularly its deep-dish pizza and Chicago-style hot dogs.\n",
      "AI Response Metadata: {'model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'timestamp': '2024-04-20 21:23:56.581797+00:00', 'like_data': None, 'hyperparameters': {'temp': 0.7, 'max_tokens': 1000}}\n",
      "Current chat history:\n",
      "[SystemMessage(content='You are a helpful assistant.'), HumanMessage(content='What is the capital of Illinois?'), AIMessage(content=\"The capital city of Illinois is Springfield. It's located in the central part of the state and is the county seat of Sangamon County. Springfield is known for its rich history, including being the site of Abraham Lincoln's presidential library and his former home, which is now a National Historic Site.\", response_metadata={'model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'timestamp': '2024-04-20 21:23:52.736214+00:00', 'like_data': None, 'hyperparameters': {'temp': 0.7, 'max_tokens': 1000}}, id='run-612361f8-f940-42b4-b4a5-908c9a415c28-0'), HumanMessage(content='What is the largest city in that state?'), AIMessage(content=\"The largest city in Illinois is Chicago. Chicago is located in the northeastern part of the state and is the third most populous city in the United States. It's known for its iconic skyline, major industries, cultural institutions, and its role as a global hub for commerce, transportation, and technology. Chicago is also famous for its architecture, museums, and its food scene, particularly its deep-dish pizza and Chicago-style hot dogs.\", response_metadata={'model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'timestamp': '2024-04-20 21:23:56.581797+00:00', 'like_data': None, 'hyperparameters': {'temp': 0.7, 'max_tokens': 1000}}, id='run-de195be3-bb0b-4607-b3fc-d5b3374849e9-0')]\n"
     ]
    }
   ],
   "source": [
    "# Instantiating a simple chat history\n",
    "chat_history = ChatMessageHistory(messages = [\n",
    "    SystemMessage(content = 'You are a helpful assistant.')\n",
    "])\n",
    "\n",
    "# Generating the response with the first prompt\n",
    "prompt_text = 'What is the capital of Illinois?'\n",
    "response = inference_chain.invoke({\n",
    "    'history': chat_history.messages,\n",
    "    'input': prompt_text\n",
    "})\n",
    "print(f'First prompt: \"{prompt_text}')\n",
    "print(f'AI Response: {response.content}\"')\n",
    "print(f'AI Response Metadata: {response.response_metadata}')\n",
    "print('Current chat history:')\n",
    "print(chat_history.messages)\n",
    "\n",
    "print('\\n-------------\\n')\n",
    "\n",
    "# Generating the response with the second prompt\n",
    "prompt_text = 'What is the largest city in that state?'\n",
    "response = inference_chain.invoke({\n",
    "    'history': chat_history.messages,\n",
    "    'input': prompt_text\n",
    "})\n",
    "print(f'Second prompt: \"{prompt_text}\"')\n",
    "print(f'AI Response: {response.content}')\n",
    "print(f'AI Response Metadata: {response.response_metadata}')\n",
    "print('Current chat history:')\n",
    "print(chat_history.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant.'),\n",
       " HumanMessage(content='What is the capital of Illinois?'),\n",
       " AIMessage(content=\"The capital city of Illinois is Springfield. It's located in the central part of the state and is the county seat of Sangamon County. Springfield is known for its rich history, including being the site of Abraham Lincoln's presidential library and his former home, which is now a National Historic Site.\", response_metadata={'model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'timestamp': '2024-04-20 21:23:52.736214+00:00', 'like_data': None, 'hyperparameters': {'temp': 0.7, 'max_tokens': 1000}}, id='run-612361f8-f940-42b4-b4a5-908c9a415c28-0'),\n",
       " HumanMessage(content='What is the largest city in that state?'),\n",
       " AIMessage(content=\"The largest city in Illinois is Chicago. Chicago is located in the northeastern part of the state and is the third most populous city in the United States. It's known for its iconic skyline, major industries, cultural institutions, and its role as a global hub for commerce, transportation, and technology. Chicago is also famous for its architecture, museums, and its food scene, particularly its deep-dish pizza and Chicago-style hot dogs.\", response_metadata={'model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'timestamp': '2024-04-20 21:23:56.581797+00:00', 'like_data': None, 'hyperparameters': {'temp': 0.7, 'max_tokens': 1000}}, id='run-de195be3-bb0b-4607-b3fc-d5b3374849e9-0')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the Conversation History Schema\n",
    "As mentioned before, we are going to be emulating the structure of the schema as defined in `data/schema.json`. In this notebook, we are going to pretend as if the user is a brand new user, so we will need to set up the conversation history schema from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ab414a6d-9301-4cd7-b48d-bee49016f28d'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating a current conversation ID\n",
    "current_conversation_id = str(uuid.uuid4())\n",
    "current_conversation_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': 'default_username',\n",
       " 'conversation_history': [{'conversation_id': 'ab414a6d-9301-4cd7-b48d-bee49016f28d',\n",
       "   'summary_title': '',\n",
       "   'conversation': [{'role': 'system',\n",
       "     'content': 'You are a helpful assistant.'}]}]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the base conversation history schema per a single user\n",
    "BASE_CONVERSATION_HISTORY_SCHEMA = {\n",
    "    'user_id': 'default_username',\n",
    "    'conversation_history': []\n",
    "}\n",
    "\n",
    "BASE_CONVERSATION_HISTORY_SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the user history from the base schema\n",
    "user_history = BASE_CONVERSATION_HISTORY_SCHEMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating a First Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the LangChain Objects to and from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting a New (Second) Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
