{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Chat History Management\n",
    "In the [LangChain Expression Language (LCEL)](./lcel.ipynb), we covered LCEL at a high level, demonstrating specifically how to chain a prompt engineered chat prompt with an LLM, namely MLX. What I failed to demonstrate in that notebook was how to think about memory (aka chat conversation management), and to be honest, I underestimated how \"involved\" of a thing this got to be! ðŸ˜… To be clear, what we will be covering in this notebook is less of a technical concern and more of a business logic concern.\n",
    "\n",
    "While LangChain offers many mechanisms for handling chat conversations (aka memory) correctly, I found some of the higher level ones to not be satisfactory for our purposes. Specifically, since I want us to adhere to a fixed schema, the high level abstraction objects provided by LangChain simply don't operate in the ideal way in which we need them to. No worries! We can still work around this without having to abandon LangChain. We're just going to need to do some special stuff throughout this notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Level Flow\n",
    "Before we get into the code itself, let's talk about how we want to think about the flow. For simplicity's sake, we are going to be ultimately saving this chat history as a JSON file. This JSON file should look like the schema that we've defined in the file `data/schema.json`.\n",
    "\n",
    "Let's say that the user is loading the MLX Gradio UI interface, either for the first time ever or as a returning user. Here is the flow of how we should be thinking about our data:\n",
    "\n",
    "1. **Loading the chat history from file**: Just as it sounds, we will want to load the chat history from file so that the user can interact with their historical conversations if they would like. Now, it's possible that this is the user's first time interacting with the chatbot, so it may be that we need to create this file from scratch!\n",
    "2. **Setting a new conversation ID**: Regardless if the user is new or returning, we are going to make the assumption that the user will want to begin with a new conversation. This means that we will need to instantiate a new conversation ID so that we can keep appending new conversation interactions to that same conversation thread.\n",
    "3. **Managing conversation back-and-forth**: As the conversation proceeds, we will want to continually update our conversation schema with any new human and AI interactions. This will include also autosaving them to file for the user's convenience.\n",
    "4. **Starting a new conversation / loading an existing conversation**: At any point, the user may want to pivot from their current conversation to either a new conversation or to continue another historical conversation loaded from our file as part of step 1. If this is the case, we will need to ensure that our backend system is referencing the correct conversation interaction.\n",
    "\n",
    "To really drive home the point, we will actually jump back and forth between each of these use cases to ensure that everything works seamlessly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "In this section, we'll do all our usual set ups. We'll also set up the LangChain MLX model using the new ChatMLX implementation. All these are things we've already explored in other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary Python libraries\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.llms.mlx_pipeline import MLXPipeline\n",
    "from langchain_community.chat_models.mlx import ChatMLX\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_community.chat_message_histories.in_memory import ChatMessageHistory\n",
    "from langchain_core.messages.human import HumanMessage\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "from langchain_core.messages.system import SystemMessage\n",
    "from langchain_core.prompt_values import ChatPromptValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a default system prompt\n",
    "DEFAULT_SYSTEM_PROMPT = 'You are a helpful assistant.'\n",
    "\n",
    "class MLXModelParameters():\n",
    "\n",
    "    def __init__(self, temp = 0.7, max_tokens = 1000, system_prompt = DEFAULT_SYSTEM_PROMPT):\n",
    "        self.temp = temp\n",
    "        self.max_tokens = max_tokens\n",
    "        self.system_prompt = system_prompt\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'Temperature: {self.temp}\\nMax Tokens: {self.max_tokens}\\nSystem Prompt: {self.system_prompt}'\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'Temperature: {self.temp}\\nMax Tokens: {self.max_tokens}\\System Prompt: {self.system_prompt}'\n",
    "\n",
    "    def update_temp(self, new_temp):\n",
    "        self.temp = new_temp\n",
    "\n",
    "    def update_max_tokens(self, new_max_tokens):\n",
    "        self.max_tokens = new_max_tokens\n",
    "\n",
    "    def update_system_prompt(self, new_system_prompt):\n",
    "        self.system_prompt = new_system_prompt\n",
    "\n",
    "    def to_json(self):\n",
    "        return { 'temp': self.temp, 'max_tokens': self.max_tokens }\n",
    "    \n",
    "mlx_model_parameters = MLXModelParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a starter conversation history object\n",
    "STARTER_CONVERSATION_HISTORY = ChatMessageHistory(messages = [ SystemMessage(content = 'You are a helpful assistant.') ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a list of models that we'll need to check against\n",
    "NO_SYSTEM_MODEL_PROVIDERS = ['mistralai', 'meta-llama']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting constant values to represent model name and directory\n",
    "MODEL_NAME = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "BASE_DIRECTORY = '../models'\n",
    "MLX_DIRECTORY = f'{BASE_DIRECTORY}/mlx'\n",
    "mlx_model_directory = f'{MLX_DIRECTORY}/{MODEL_NAME}'\n",
    "\n",
    "# Setting a constant value to represent where to place the chat history data\n",
    "CHAT_HISTORY_DIRECTORY = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Setting up the LangChain MLX LLM\n",
    "llm = MLXPipeline.from_model_id(\n",
    "    model_id = mlx_model_directory,\n",
    "    pipeline_kwargs = {\n",
    "        'temp': mlx_model_parameters.temp,\n",
    "        'max_tokens': mlx_model_parameters.max_tokens,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Setting up the LangChain MLX Chat Model with the LLM above\n",
    "chat_model = ChatMLX(llm = llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the LangChain inference pipeline\n",
    "In order to use LangChain's preferred implementation of memory management, we're first going to need to establish our LangChain pipeline. We've done similar things to this in other notebooks, but with this particular implementation, we are going to make a specific adjustment. Namely, since we are now going to make use of the LangChain Community implementation of MLX, we are going to need to manually add our own metadata. To seamlessly do this, we are going to make use of LCEL's **RunnableLambda**, which essentially allows us to define our own custom function.\n",
    "\n",
    "Also note that when we set up our chat prompt, we are going to need to slide in an extra entry referred to as **MessagesPlaceholder**. As the name implies, that will serve as a placeholder so that we can keep passing the history back through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the Chat prompt template\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(template = \"{input}\")\n",
    "chat_prompt = ChatPromptTemplate.from_messages(messages = [\n",
    "    MessagesPlaceholder(variable_name = 'history'),\n",
    "    human_message_prompt\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_for_no_system_models(chat_history):\n",
    "    '''\n",
    "    Precorrects for the issue where certain models (e.g. Llama, Mistral) are unable to accept for system messages\n",
    "\n",
    "    Inputs:\n",
    "        - chat_history (LangChain ChatPromptValue): The current chat history with no alterations\n",
    "\n",
    "    Returns:\n",
    "        - chat_history (LangChain ChatPromptValue): The new chat history with alterations (if needed)\n",
    "    '''\n",
    "    # Referencing global variables\n",
    "    global MODEL_NAME\n",
    "    global NO_SYSTEM_MODEL_PROVIDERS\n",
    "\n",
    "\n",
    "    # Checking if the correction needs to be made if the model is Llama or Mistral\n",
    "    if MODEL_NAME.split('/')[0] in NO_SYSTEM_MODEL_PROVIDERS:\n",
    "\n",
    "        # Getting the system message content\n",
    "        system_message_content = chat_history.messages[0].content\n",
    "\n",
    "        # Replacing the System Message with a Human Message\n",
    "        chat_history.messages[0] = HumanMessage(content = system_message_content)\n",
    "\n",
    "        # Adds a dummy AI Message\n",
    "        chat_history.messages.insert(1, AIMessage(content = ''))\n",
    "\n",
    "    return chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_ai_response_metadata(ai_message):\n",
    "    '''\n",
    "    Updates the metadata on the AI response\n",
    "\n",
    "    Inputs:\n",
    "        - ai_message (LangChain AIMessage): The AI message produced by the model\n",
    "\n",
    "    Returns:\n",
    "        - ai_message (LangChain AIMessage): The AI message produced by the model, except now with the appropriate metadata intact\n",
    "    '''\n",
    "\n",
    "    # Referencing global variables\n",
    "    global mlx_model_parameters\n",
    "    global MODEL_NAME\n",
    "\n",
    "    # Creating a dictionary of the metadata that we will be adding to the AI message\n",
    "    metadata = {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'timestamp': str(pd.Timestamp.utcnow()),\n",
    "        'like_data': None,\n",
    "        'hyperparameters': mlx_model_parameters.to_json()\n",
    "    }\n",
    "\n",
    "    # Applying the metadata to the AI response\n",
    "    ai_message.response_metadata = metadata\n",
    "\n",
    "    return ai_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_chat_history(ai_message):\n",
    "    '''\n",
    "    Adds messages to chat history\n",
    "\n",
    "    Inputs:\n",
    "        - ai_message (LangChain AIMessage): The AI message produced by the AI model\n",
    "\n",
    "    Returns:\n",
    "        - ai_message (LangChain AIMessage): Passing the AI message through to the end of the inference pipeline\n",
    "    '''\n",
    "\n",
    "    # Referencing global variables\n",
    "    global chat_history\n",
    "    global prompt_text\n",
    "\n",
    "    # Adding the AI message to the chat history\n",
    "    chat_history.add_messages(\n",
    "        messages = [\n",
    "            HumanMessage(content = prompt_text),\n",
    "            ai_message\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return ai_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the inference chain by chaining together the chat prompt, chat model, and custom function to update metadata\n",
    "inference_chain = chat_prompt | RunnableLambda(correct_for_no_system_models) | chat_model | RunnableLambda(update_ai_response_metadata) | RunnableLambda(add_to_chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First prompt: \"What is the capital of Illinois?\n",
      "AI Response: The capital city of Illinois is Springfield. It's located in the central part of the state and is the county seat of Sangamon County. Springfield is known for its rich history, including being the site of Abraham Lincoln's presidential library and his former home, which is now a National Historic Site.\"\n",
      "AI Response Metadata: {'model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'timestamp': '2024-04-21 04:49:47.984104+00:00', 'like_data': None, 'hyperparameters': {'temp': 0.7, 'max_tokens': 1000}}\n",
      "Current chat history:\n",
      "[SystemMessage(content='You are a helpful assistant.'), HumanMessage(content='What is the capital of Illinois?'), AIMessage(content=\"The capital city of Illinois is Springfield. It's located in the central part of the state and is the county seat of Sangamon County. Springfield is known for its rich history, including being the site of Abraham Lincoln's presidential library and his former home, which is now a National Historic Site.\", response_metadata={'model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'timestamp': '2024-04-21 04:49:47.984104+00:00', 'like_data': None, 'hyperparameters': {'temp': 0.7, 'max_tokens': 1000}}, id='run-2b97edc4-2510-492a-af12-d63ad4a95e62-0')]\n",
      "\n",
      "-------------\n",
      "\n",
      "Second prompt: \"What is the largest city in that state?\"\n",
      "AI Response: The largest city in Illinois is Chicago. Chicago is located in the northeastern part of the state and is the third most populous city in the United States. It's known for its iconic skyline, major industries, cultural institutions, and its role as a global hub for commerce, transportation, and technology. Chicago is also famous for its architecture, museums, and its food scene, particularly its deep-dish pizza and Chicago-style hot dogs.\n",
      "AI Response Metadata: {'model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'timestamp': '2024-04-21 04:49:51.668462+00:00', 'like_data': None, 'hyperparameters': {'temp': 0.7, 'max_tokens': 1000}}\n",
      "Current chat history:\n",
      "[SystemMessage(content='You are a helpful assistant.'), HumanMessage(content='What is the capital of Illinois?'), AIMessage(content=\"The capital city of Illinois is Springfield. It's located in the central part of the state and is the county seat of Sangamon County. Springfield is known for its rich history, including being the site of Abraham Lincoln's presidential library and his former home, which is now a National Historic Site.\", response_metadata={'model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'timestamp': '2024-04-21 04:49:47.984104+00:00', 'like_data': None, 'hyperparameters': {'temp': 0.7, 'max_tokens': 1000}}, id='run-2b97edc4-2510-492a-af12-d63ad4a95e62-0'), HumanMessage(content='What is the largest city in that state?'), AIMessage(content=\"The largest city in Illinois is Chicago. Chicago is located in the northeastern part of the state and is the third most populous city in the United States. It's known for its iconic skyline, major industries, cultural institutions, and its role as a global hub for commerce, transportation, and technology. Chicago is also famous for its architecture, museums, and its food scene, particularly its deep-dish pizza and Chicago-style hot dogs.\", response_metadata={'model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'timestamp': '2024-04-21 04:49:51.668462+00:00', 'like_data': None, 'hyperparameters': {'temp': 0.7, 'max_tokens': 1000}}, id='run-d98a7d7f-41c6-4cd8-959b-ced4ac0b2094-0')]\n"
     ]
    }
   ],
   "source": [
    "# Instantiating a simple chat history\n",
    "chat_history = ChatMessageHistory(messages = [\n",
    "    SystemMessage(content = 'You are a helpful assistant.')\n",
    "])\n",
    "\n",
    "# Generating the response with the first prompt\n",
    "prompt_text = 'What is the capital of Illinois?'\n",
    "response = inference_chain.invoke({\n",
    "    'history': chat_history.messages,\n",
    "    'input': prompt_text\n",
    "})\n",
    "print(f'First prompt: \"{prompt_text}')\n",
    "print(f'AI Response: {response.content}\"')\n",
    "print(f'AI Response Metadata: {response.response_metadata}')\n",
    "print('Current chat history:')\n",
    "print(chat_history.messages)\n",
    "\n",
    "print('\\n-------------\\n')\n",
    "\n",
    "# Generating the response with the second prompt\n",
    "prompt_text = 'What is the largest city in that state?'\n",
    "response = inference_chain.invoke({\n",
    "    'history': chat_history.messages,\n",
    "    'input': prompt_text\n",
    "})\n",
    "print(f'Second prompt: \"{prompt_text}\"')\n",
    "print(f'AI Response: {response.content}')\n",
    "print(f'AI Response Metadata: {response.response_metadata}')\n",
    "print('Current chat history:')\n",
    "print(chat_history.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant.'),\n",
       " HumanMessage(content='What is the capital of Illinois?'),\n",
       " AIMessage(content=\"The capital city of Illinois is Springfield. It's located in the central part of the state and is the county seat of Sangamon County. Springfield is known for its rich history, including being the site of Abraham Lincoln's presidential library and his former home, which is now a National Historic Site.\", response_metadata={'model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'timestamp': '2024-04-21 04:49:47.984104+00:00', 'like_data': None, 'hyperparameters': {'temp': 0.7, 'max_tokens': 1000}}, id='run-2b97edc4-2510-492a-af12-d63ad4a95e62-0'),\n",
       " HumanMessage(content='What is the largest city in that state?'),\n",
       " AIMessage(content=\"The largest city in Illinois is Chicago. Chicago is located in the northeastern part of the state and is the third most populous city in the United States. It's known for its iconic skyline, major industries, cultural institutions, and its role as a global hub for commerce, transportation, and technology. Chicago is also famous for its architecture, museums, and its food scene, particularly its deep-dish pizza and Chicago-style hot dogs.\", response_metadata={'model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'timestamp': '2024-04-21 04:49:51.668462+00:00', 'like_data': None, 'hyperparameters': {'temp': 0.7, 'max_tokens': 1000}}, id='run-d98a7d7f-41c6-4cd8-959b-ced4ac0b2094-0')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Generating a Summary Title\n",
    "In certain interfaces including OpenAI's ChatGPT, the chat history will represent your conversation with what I like to call a \"summary title.\" For example, let's say I ask it for a recipe for chocolate chip cookies, then the ChatGPT interface will represent my conversation in the chat history window as something like \"A Recipe for Chocolate Chip Cookies\". While this is not necessary, I thought it might be a fun touch to add!\n",
    "\n",
    "Let's demonstrate by starting a conversation asking it to write a fun haiku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \"Please give me a recipe for delicions chocolate chip cookies.\n",
      "AI Response: I'd be happy to help you make delicious chocolate chip cookies! Here's a simple and popular recipe that you can try at home.\n",
      "\n",
      "Ingredients:\n",
      "- 1 cup (2 sticks) unsalted butter, softened\n",
      "- 1 cup white sugar\n",
      "- 1 cup packed brown sugar\n",
      "- 2 eggs\n",
      "- 2 teaspoons vanilla extract\n",
      "- 3 1/2 cups all-purpose flour\n",
      "- 1\"\n",
      "AI Response Metadata: {'model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'timestamp': '2024-04-21 04:49:55.055835+00:00', 'like_data': None, 'hyperparameters': {'temp': 0.7, 'max_tokens': 1000}}\n",
      "Current chat history:\n",
      "[SystemMessage(content='You are a helpful assistant.'), HumanMessage(content='Please give me a recipe for delicions chocolate chip cookies.'), AIMessage(content=\"I'd be happy to help you make delicious chocolate chip cookies! Here's a simple and popular recipe that you can try at home.\\n\\nIngredients:\\n- 1 cup (2 sticks) unsalted butter, softened\\n- 1 cup white sugar\\n- 1 cup packed brown sugar\\n- 2 eggs\\n- 2 teaspoons vanilla extract\\n- 3 1/2 cups all-purpose flour\\n- 1\", response_metadata={'model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'timestamp': '2024-04-21 04:49:55.055835+00:00', 'like_data': None, 'hyperparameters': {'temp': 0.7, 'max_tokens': 1000}}, id='run-563bd21f-a65e-4de2-b001-bb5e12520efc-0')]\n"
     ]
    }
   ],
   "source": [
    "# Instantiating a simple chat history\n",
    "chat_history = ChatMessageHistory(messages = [\n",
    "    SystemMessage(content = 'You are a helpful assistant.')\n",
    "])\n",
    "\n",
    "# Generating the response with the prompt\n",
    "prompt_text = 'Please give me a recipe for delicions chocolate chip cookies.'\n",
    "response = inference_chain.invoke({\n",
    "    'history': chat_history.messages,\n",
    "    'input': prompt_text\n",
    "})\n",
    "print(f'Prompt: \"{prompt_text}')\n",
    "print(f'AI Response: {response.content}\"')\n",
    "print(f'AI Response Metadata: {response.response_metadata}')\n",
    "print('Current chat history:')\n",
    "print(chat_history.messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very much like how we created the inference chain, we can use LangChain again here to create a new chain that will produce the summary as we please! In the code below, you will see the prompt engineering that I use to generate this summary heading.\n",
    "\n",
    "(Note: You'll notice I'm using the `HumanMessagePromptTemplate` object for placing my prompt engineering. Ideally, we would place this prompt engineering in something more representative of the system message, but as you already know, some of these models don't play well with the `SystemMessage` object! In this particular case, we are not saving any metadata about how we generated this summary, so we can sneak by using the human message prompt template.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the summary title prompt engineering\n",
    "summary_title_prompt = '''The text delineated by the triple backticks below contains the beginning of a conversation between a human and a large language model (LLM). Please provide a brief summary to serve as a title for this conversation. Do not use any system messages. Place more emphasis on the human's prompt over the AI's response. Please ensure the summary title does not exceed more than ten words. Please format the summary title as one would any formal title, like that of a book. Do not give any extra words except the summary title. (Example: Do not show \"Title: \")\n",
    "\n",
    "```\n",
    "{history}\n",
    "```\n",
    "'''\n",
    "\n",
    "# Creating the LangChain chat prompt\n",
    "summary_title_prompt_template = HumanMessagePromptTemplate.from_template(template = summary_title_prompt)\n",
    "\n",
    "summary_title_chat_prompt = ChatPromptTemplate.from_messages(messages = [\n",
    "    summary_title_prompt_template\n",
    "])\n",
    "\n",
    "# Creating the summary title chain\n",
    "summary_title_chain = summary_title_chat_prompt | chat_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Title: Title: Delicious Chocolate Chip Cookies Recipe\n",
      "\n",
      "(Note: The title focuses on the human's request for a chocolate chip cookies recipe and does not include any details about the AI's response.)\n"
     ]
    }
   ],
   "source": [
    "# Generating the summary title based on the sample chat history\n",
    "summary_title_response = summary_title_chain.invoke({\n",
    "    'history': chat_history.messages\n",
    "})\n",
    "\n",
    "print(f'Summary Title: {summary_title_response.content}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delicious Chocolate Chip Cookies Recipe\n",
      "\n",
      "(Note: The title focuses on the human's request for a chocolate chip cookies recipe and does not include any details about the AI's response.)\n"
     ]
    }
   ],
   "source": [
    "# Stripping out \"Title: \" (Because no idea why it wants to keep doing that...)\n",
    "summary_title = summary_title_response.content.replace(\"Title: \", \"\").replace(\"title: \", \"\")\n",
    "print(summary_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the Conversation History Schema\n",
    "As mentioned before, we are going to be emulating the structure of the schema as defined in `data/schema.json`. In this notebook, we are going to pretend as if the user is a brand new user, so we will need to set up the conversation history schema from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conv_id_91496830_918e_4934_a7ba_1420346912ac'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating a current conversation ID\n",
    "current_conversation_id = 'conv_id_' + str.replace(str(uuid.uuid4()), '-', '_')\n",
    "current_conversation_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': 'default_username', 'chat_history': []}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the base conversation history schema per a single user\n",
    "BASE_CONVERSATION_HISTORY_SCHEMA = {\n",
    "    'user_id': 'default_username',\n",
    "    'chat_history': []\n",
    "}\n",
    "\n",
    "BASE_CONVERSATION_HISTORY_SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the user history from the base schema\n",
    "user_history = BASE_CONVERSATION_HISTORY_SCHEMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the LangChain Messages to and from JSON\n",
    "One great thing about LangChain are the many integrations it offers for connections to many of your favorite online services. To keep things simple, I want to save my chat history as a JSON file. Now as you saw in the previous section, I'm a bit picky with my choice of schema. This means that we are going to need to figure out a way to convert the LangChain messages to and from JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lc_to_json_list(lc_messages):\n",
    "    '''\n",
    "    Converts LangChain messages to a JSON-like structure in a list\n",
    "\n",
    "    Inputs:\n",
    "        - lc_messages (List[BaseMessage]): A list of LangChain message objects\n",
    "\n",
    "    Returns:\n",
    "        - conversation_json_list (list): The LangChain messages now represented in our JSON structure\n",
    "    '''\n",
    "\n",
    "    # Instantiating a dictionary to hold the JSON output\n",
    "    conversation_json_list = []\n",
    "\n",
    "    # Iterating over each of the LangChain messages\n",
    "    for message in lc_messages:\n",
    "\n",
    "        # Determining the action based on if message is System type\n",
    "        if message.type == 'system':\n",
    "\n",
    "            conversation_json = {\n",
    "                'role': 'system',\n",
    "                'content': message.content\n",
    "            }\n",
    "            conversation_json_list.append(conversation_json)\n",
    "\n",
    "        # Determining the action based on if message is Human type\n",
    "        elif message.type == 'human':\n",
    "\n",
    "            conversation_json = {\n",
    "                'role': 'user',\n",
    "                'content': message.content,\n",
    "            }\n",
    "            conversation_json_list.append(conversation_json)\n",
    "        \n",
    "        # Determining the action based on if message is AI type\n",
    "        elif message.type == 'ai':\n",
    "\n",
    "            conversation_json = {\n",
    "                'role': 'assistant',\n",
    "                'content': message.content,\n",
    "                'metadata': message.response_metadata\n",
    "            }\n",
    "            conversation_json_list.append(conversation_json)\n",
    "        \n",
    "    return conversation_json_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a helpful assistant.'},\n",
       " {'role': 'user',\n",
       "  'content': 'Please give me a recipe for delicions chocolate chip cookies.'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"I'd be happy to help you make delicious chocolate chip cookies! Here's a simple and popular recipe that you can try at home.\\n\\nIngredients:\\n- 1 cup (2 sticks) unsalted butter, softened\\n- 1 cup white sugar\\n- 1 cup packed brown sugar\\n- 2 eggs\\n- 2 teaspoons vanilla extract\\n- 3 1/2 cups all-purpose flour\\n- 1\",\n",
       "  'metadata': {'model_name': 'mistralai/Mistral-7B-Instruct-v0.2',\n",
       "   'timestamp': '2024-04-21 04:49:55.055835+00:00',\n",
       "   'like_data': None,\n",
       "   'hyperparameters': {'temp': 0.7, 'max_tokens': 1000}}}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the LangChain history to a JSON-like list of messages\n",
    "conversation_json_list = lc_to_json_list(chat_history.messages)\n",
    "conversation_json_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': 'default_username',\n",
       " 'chat_history': [{'summary_title': \"Delicious Chocolate Chip Cookies Recipe\\n\\n(Note: The title focuses on the human's request for a chocolate chip cookies recipe and does not include any details about the AI's response.)\",\n",
       "   'conversation_id': 'conv_id_91496830_918e_4934_a7ba_1420346912ac',\n",
       "   'conversation': [{'role': 'system',\n",
       "     'content': 'You are a helpful assistant.'},\n",
       "    {'role': 'user',\n",
       "     'content': 'Please give me a recipe for delicions chocolate chip cookies.'},\n",
       "    {'role': 'assistant',\n",
       "     'content': \"I'd be happy to help you make delicious chocolate chip cookies! Here's a simple and popular recipe that you can try at home.\\n\\nIngredients:\\n- 1 cup (2 sticks) unsalted butter, softened\\n- 1 cup white sugar\\n- 1 cup packed brown sugar\\n- 2 eggs\\n- 2 teaspoons vanilla extract\\n- 3 1/2 cups all-purpose flour\\n- 1\",\n",
       "     'metadata': {'model_name': 'mistralai/Mistral-7B-Instruct-v0.2',\n",
       "      'timestamp': '2024-04-21 04:49:55.055835+00:00',\n",
       "      'like_data': None,\n",
       "      'hyperparameters': {'temp': 0.7, 'max_tokens': 1000}}}]}]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding the conversation to the user's overall history\n",
    "user_history['chat_history'].append({\n",
    "    'summary_title': summary_title,\n",
    "    'conversation_id': current_conversation_id,\n",
    "    'conversation': conversation_json_list\n",
    "})\n",
    "user_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the chat history to file\n",
    "with open('../data/chat_history.json', 'w') as f:\n",
    "    json.dump(user_history, f, indent = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_list_to_lc(conversation_json_list):\n",
    "    '''\n",
    "    Converts the JSON-like list of conversation messages into a formalized LangChain chat history\n",
    "\n",
    "    Inputs:\n",
    "        - conversation_json_list (List[dict]): A JSON-like list of the conversation messages\n",
    "\n",
    "    Returns:\n",
    "        - chat_history (LangChain ChatMessageHistory): The input list but now in LangChain form\n",
    "    '''\n",
    "\n",
    "    # Instantiating the chat history\n",
    "    chat_history = ChatMessageHistory()\n",
    "\n",
    "    # Iterating over all the messages\n",
    "    for message in conversation_json_list:\n",
    "\n",
    "        # Taking action based on the message's role type\n",
    "        if message['role'] == 'system':\n",
    "            chat_history.add_message(SystemMessage(content = message['content']))\n",
    "        elif message['role'] == 'user':\n",
    "            chat_history.add_message(HumanMessage(content = message['content']))\n",
    "        elif message['role'] == 'assistant':\n",
    "            chat_history.add_message(AIMessage(content = message['content'], response_metadata = message['metadata']))\n",
    "    \n",
    "    return chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading user history from file\n",
    "with open('../data/chat_history.json') as f:\n",
    "    user_history = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the chat history from the first conversation of the user history\n",
    "chat_history = json_list_to_lc(user_history['chat_history'][0]['conversation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant.'),\n",
       " HumanMessage(content='Please give me a recipe for delicions chocolate chip cookies.'),\n",
       " AIMessage(content=\"I'd be happy to help you make delicious chocolate chip cookies! Here's a simple and popular recipe that you can try at home.\\n\\nIngredients:\\n- 1 cup (2 sticks) unsalted butter, softened\\n- 1 cup white sugar\\n- 1 cup packed brown sugar\\n- 2 eggs\\n- 2 teaspoons vanilla extract\\n- 3 1/2 cups all-purpose flour\\n- 1\", response_metadata={'model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'timestamp': '2024-04-21 04:49:55.055835+00:00', 'like_data': None, 'hyperparameters': {'temp': 0.7, 'max_tokens': 1000}})]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the re-loaded messages\n",
    "chat_history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Real Life Use Case\n",
    "Okay, we now have the essential basic framework to start using our chatbot and saving our chat history to a local JSON file. We will jump back and forth between different use cases to ensure that what we are building is resilient to everyday use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating a First Conversation\n",
    "Let's begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting a New (Second) Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jumping Back to First Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading It All Back In from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting a Third (Final) Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returning to the First Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
